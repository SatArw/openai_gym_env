{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a468d2",
   "metadata": {},
   "source": [
    "RL Notes:\n",
    "\n",
    "Focuses on teaching through trial and error\n",
    "\n",
    "Fundamental elements:\n",
    "\n",
    "1. Agent - Something that operates in the environment, could be a player or a model\n",
    "\n",
    "2. Environment\n",
    "\n",
    "3. Action - Doing something in the environment (done by agent)\n",
    "\n",
    "4. Rewards - Returns on an action\n",
    "\n",
    "RL assumes our environment follows the Markov property, given the present, the probability of the future is independent of the past (this property is also called “memoryless property”)\n",
    "\n",
    "I will be working on Model-free RL in this notebook : PPO and DQN \n",
    "\n",
    "Core metrics to look at:\n",
    "1. Average reward\n",
    "\n",
    "2. Average episode length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c45700",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install stable-baselines3[extra] \n",
    "#pretty cool RL library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad04ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv #somehow increases training speed and efficiency\n",
    "from stable_baselines3.common.evaluation import evaluate_policy #returns some parameters after running\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22093b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9bbfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using openAI gym for environments\n",
    "env_name = \"CartPole-v0\"\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interacting and getting familiar with the environment\n",
    "eps = 5\n",
    "for i in range(1,eps+1):\n",
    "    state = env.reset() #returns a list of values that determine a state of the environment\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() #graphically renders the environment\n",
    "        action = env.action_space.sample() #picks a random action to be taken\n",
    "        n_state, reward, done, info = env.step(action) #takes the action using the .step method, \n",
    "        #new state is returned in n_state, and the same goes for reward. done returns when cartpole hits the ground.\n",
    "        score += reward \n",
    "    print('Episode: {} score: {}'.format(i,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24033770",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space\n",
    "# This says that we have discrete 2 actions indexed by 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space\n",
    "# This says we have a box type framework with 4 values in the range of first two values in output and the last one tells us the type of values\n",
    "# The 4 values are cart position and velocity and pole position and its angular velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f14026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making log path\n",
    "log_path = os.path.join(\"Training\",\"logs\")\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aee0d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating a PPO model\n",
    "env = gym.make(env_name)\n",
    "env = DummyVecEnv([lambda: env]) #wrapping env in DummyVecEnv \n",
    "model = PPO('MlpPolicy',env,verbose=1,tensorboard_log = log_path) #its like defining the agent\n",
    "#First thing says we are using Multi layer perceptron policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce930d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57ebb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    model.learn(total_timesteps = 20000) #Training the model over 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b1a0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training','models','PPO_Model_CartPole')\n",
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model #just deleting so that we can load the model again from the saved path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe68d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4024adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a78f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7fb997",
   "metadata": {},
   "source": [
    "Testing summary:\n",
    "\n",
    "We first get observations for the environment using the env.reset(), set done = False and score = 0\n",
    "\n",
    "And while balancing isn't done:\n",
    "\n",
    "1. We render the env graphically\n",
    "\n",
    "2. Pick the best action using our trained model\n",
    "\n",
    "3. update rewards and state \n",
    "\n",
    "At the end of episode we output score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad12a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing \n",
    "#\n",
    "eps = 5\n",
    "for i in range(1,eps+1):\n",
    "    obs = env.reset() #returns a list of values that determine a state of the environment\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() #graphically renders the environment    \n",
    "        action, _ = model.predict(obs) #picks the best action with intel from model (this returns the action and state after action\n",
    "        obs, reward, done, info = env.step(action) #takes the action using the .step method, \n",
    "        #new state is returned in n_state, and the same goes for reward. done returns if the pole is finally balanced.\n",
    "        score += reward #reward is 1 as long as the pole doesn't fall \n",
    "    print('Episode: {} score: {}'.format(i,score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5191f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing logs from tensorboard\n",
    "training_log_path = os.path.join('Training','logs','PPO_1')\n",
    "training_log_path\n",
    "!tensorboard --logdir = {training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa81f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912acc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485cba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stops training after reward reaches 190\n",
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold=190, verbose=1) \n",
    "\n",
    "#saves the best model \n",
    "eval_callback = EvalCallback(env, \n",
    "                             callback_on_new_best=stop_callback, \n",
    "                             eval_freq=10000, \n",
    "                             best_model_save_path=save_path, \n",
    "                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfedea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=20000, callback=eval_callback) #training model with our eval_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd91cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customized parameters for model \n",
    "net_arch=[dict(pi=[64,64,64 ,64], vf=[128, 128, 128, 128])] # 4 layers of 128 units, pi for the actor, vf for the value function\n",
    "model = PPO('MlpPolicy', env, verbose = 1, policy_kwargs={'net_arch': net_arch})\n",
    "#paicy_kwargs specifies out dictionary of parameters\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cbdca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using a different algorithm apart from PPO, say DQN\n",
    "from stable_baselines3 import DQN\n",
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=20000, callback=eval_callback)\n",
    "\n",
    "dqn_path = os.path.join('Training', 'Saved Models', 'DQN_model')\n",
    "model.save(dqn_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c8fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the saved model \n",
    "model = DQN.load(dqn_path, env=env)\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefef820",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670822b0",
   "metadata": {},
   "source": [
    "Summary of notebook\n",
    "\n",
    "1. We first learnt a bit about RL and its concepts. \n",
    "\n",
    "2. stable-baselines3 is a great RL library with lots of models \n",
    "\n",
    "3. Loading the cartpole environment. Every environment has an action_space and an observation_space. The observation_space basically is a set of numbers that define its state. The action_space is a set of all actions that an agent can take in the environment. \n",
    "\n",
    "4. Interacting with cartpole env. We took some steps using the env.step() by sampling actions from the action_space (using env.action_space.sample()). Then we rendered the environment graphically using env.render()\n",
    "\n",
    "5. At this point, we got introduced to the PPO model and instantiated it. Then trained it for 20000 steps for 5 epochs. Then we tested the model, find more details on this in the testing summary markdown. Later we checked our log files using Tensorboard (which has informative graphs to depict how the avg loss, avg ep length, etc varied throughout the training).\n",
    "\n",
    "6. At this point we were introduced to some callback methods, basically we stop the training when we have reached a certain rewards limit and save the current model as the best model. \n",
    "\n",
    "7. Then we attempted to make a customized PPO model by changing the number of units in its layers. We went with 4 layers of 128 units for the agent and value function. Didn't see much of a difference in the final episode reward. The default models work pretty well.\n",
    "\n",
    "8. We used the DQN model later to check results, although it didn't perform as well as the PPO model, the avg episode reward was only around 9 whereas it is around 200 for PPO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6921f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
